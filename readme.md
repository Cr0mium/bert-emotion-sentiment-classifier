# BERT-based Emotion & Sentiment Classification (NLP)

An end-to-end **Machine Learning / AI engineering‚Äìfocused NLP project** that fine-tunes **BERT** for multi-class emotion classification, with a clean training‚Äìevaluation‚Äìinference pipeline built using **PyTorch** and **Hugging Face Transformers**.

This repository is designed to reflect **real-world ML engineering practices**: modular code structure, reproducible training, checkpointing, evaluation, and interactive inference.

---

## üîç Problem Statement

Given a short text input, predict the **underlying emotion** expressed by the sentence.

- Multi-class classification (6 emotion labels)
- Transformer-based deep learning approach
- Focus on model training, optimization, and inference (not just experimentation)

---

## üß† ML / AI Engineering Focus

This project emphasizes **how models are trained and used in practice**, not just accuracy:

- Fine-tuning a **pretrained Transformer (BERT)**
- Custom **training loop** with:
  - Learning rate warmup
  - Linear scheduler
  - Gradient clipping
- Proper **train / validation / test separation**
- **Checkpoint saving & loading** for reproducibility
- Standalone **evaluation and prediction scripts**
- GPU-aware execution (CPU / CUDA)

---

## üß∞ Tech Stack

- **Python**
- **PyTorch**
- **Hugging Face Transformers & Datasets**
- **BERT (bert-base-uncased)**
- **tqdm** (progress tracking)
- **Google Colab** (GPU training)
- **Git & GitHub**

---

## üìÇ Project Structure

```
bert-emotion-sentiment-classifier/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py      # Tokenization & DataLoader preparation
‚îÇ   ‚îú‚îÄ‚îÄ model.py        # BERT model initialization
‚îÇ   ‚îú‚îÄ‚îÄ train.py        # Training loop + checkpointing
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py     # Test set evaluation
‚îÇ   ‚îî‚îÄ‚îÄ predict.py      # Interactive inference
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/        # Saved model checkpoints (optional)
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üìä Dataset

- **Hugging Face ****emotion**** dataset**
- 6 emotion classes
- Automatically split into:
  - Train
  - Validation
  - Test

---

## üöÄ Training

The model is trained using **BERT for sequence classification** with:

- AdamW optimizer
- Learning rate warmup (10%)
- Linear decay scheduler
- Gradient clipping to stabilize training

Run training:

```bash
python src/train.py
```

During training:

- Training & validation loss are tracked
- Validation accuracy is computed each epoch
- Model checkpoints are saved per epoch

---

## üìà Evaluation

Evaluate the trained model on the test set:

```bash
python src/evaluate.py
```

**Sample Result:**

```
Test Loss: 0.0379
Test Accuracy: 0.9861
```

---

## üîÆ Inference / Prediction

Interactive, loop-based prediction using the fine-tuned model:

```bash
python src/predict.py
```

Example:

```
Enter text (or type 'exit'): I feel really excited about this project!
Predicted Emotion: joy
```

The inference pipeline:

- Loads tokenizer & trained checkpoint
- Runs model in `eval()` mode
- Disables gradients for efficiency
- Converts logits ‚Üí predicted label

---

## üß™ Key ML Concepts Demonstrated

- Transformer fine-tuning
- Logits vs probabilities
- Gradient clipping
- Learning rate scheduling
- Model checkpointing
- Torch `no_grad()` for inference
- Device-aware ML code (CPU/GPU)

---

## üë®‚Äçüíª Author

**Chinmoy Deka**\
ML / AI Engineering Enthusiast

> This project reflects my focus on **machine learning engineering**, particularly in NLP and deep learning systems. It is part of my portfolio for ML / AI engineering roles.

---

## üìå Notes

- This project prioritizes **engineering clarity and correctness** over dataset scale.
- The same pipeline can be extended to:
  - Larger datasets
  - More emotion classes
  - Deployment (API / batch inference)

---

## ‚≠ê Future Improvements

- Add experiment tracking (e.g., TensorBoard)
- Hyperparameter configuration via YAML
- Model export for deployment
- Support for larger emotion taxonomies

---

